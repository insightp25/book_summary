01. pull vs. push for low latency
가독성 가독성 가독성

# 서문

대용량 데이터와 대규모 트래픽 환경에서 낮은 지연시간(latency)을 보장하는 방법

~~왜 AI와 디지털 트윈에 매료됐는지 구체적인 내용 1~2줄로 설명 필요~~

대학원에서 인공지능과 디지털 트윈이라는 기술에 대해 학습하고 연구한 적이 있고, 지금도 두 주제는 나에게 가장 큰 관심사이다.

인공지능의 학습에 필요한 빅데이터와 디지털 트윈 구현을 위한 실시간 대용량 데이터와 대규모 트래픽 처리 과정에, 특히 데이터에 집중하여, 보다 하드코어하게 부딪혀 보고 집중하고 딥다이브 해보고 싶었고, 이에 실시간 대규모 데이터 처리로 대표되며 평소 친숙한 서비스인 트위터를 모티브로 하는 소셜 네트워크 서비스(이하 SNS)를 프로젝트 주제로 선택하였다.


<br>

# 목차


<br>

# 문제 정의


<br>

## 트위터 실제 서비스의 주요 성능 요구사항

시작에 앞서 트래픽의 규모를 가늠하기 위해 수집한 통계자료의 트위터 성능 관련 주요 지표들은 다음과 같다.

* `2억 3780만 mDAU` - 유료화 일간 활성 사용자수monetizable daily active users, 2022년 기준) [1]
* `초당 450,000회 읽기 요청` - 타임라인 또는 모아보기, 트윗 10개 한 번에 로드, 2013년 기준이며 최근의 수치는 공개되지 않은 듯 하나 쓰기 비율이 거의 비슷하다고 가정 [2]
* `초당 6,000회 쓰기 요청` - 트윗, 2022년 기준 [3]
* `75 : 1` - 읽기 요청 vs. 쓰기 요청 비율
(타임라인 읽기시 트윗 10개를 한 번에 로드한다고 가정하면 러프하게 `750 : 1`이라고 간주할 수도 있을 것 같다. 통상적으로 SNS의 읽기 쓰기 비율은 100:1 ~ 10,000:1라고 한다.)
* `5초 이내` - 읽기 및 쓰기 요청에 대한 응답 지연 시간(latency) 상한

너무 큰 숫자라 잘 와닿지 않을 수가 있을 것 같아, 국내 한 유명 서비스 배달의 민족의 트래픽 기록과 비교하자면
* `초당 50,000회 읽기 요청` ('배달의 민족' 서비스의 연간 프로모션 행사에서 발생한 역대급 트래픽이었다고 한다)
* `초당 1,000회 ~ 1,500회 쓰기 요청` (동일 서비스의 MySQL 노드 한 대가 받아들일 수 있는 쓰기 요청 상한선이라고 한다)

과 같다. 읽기 요청 규모를 기준으로 했을 때, 트위터에서 약 `10~100배` 넘는 트래픽이 '일상적'으로 발생하고 있는 것이다.

모아보기


<br>

## 문제1 - SNS의 공통 요구사항 - 읽기 집중, 짧은 지연시간

SNS의 공통 핵심 요구사항은 Facebook(현 메타)의 CEO 마크 저커버그를 모티브로 한 영화 'The Social Network'의 한 유명한 대사에 잘 나타나 있다이다.

![](https://velog.velcdn.com/images/rmndr/post/14a8bbe3-7dad-46e9-a727-a40b5e7c64fa/image.jpg) 
> "우리 페이스북과 다른 서비스들의 차이는, **<u>우리 서버는 절대 멈춰선 안된다는 겁니다</u>**! 만약 서버가 하루라도 동작하지 않으면, 우리 서비스 평판은 순식간에 회생불능이 되어버릴 거에요. 유저들은 서로 연결돼 있기 때문에 일부의 사용자만 떠나도 그 여파는 금새 전체 사용자들에게 퍼지게 돼요. 그게 핵심입니다. 사용자들은 지인들이 접속해 있기 때문에 접속하는 것이고, 만약 한 사용자가 이탈하면 다른 사용자도 이탈하기 시작할 것이며, 그렇게 도미노처럼 한 순간에 무너지게 되는 거에요!"

대사에서 처럼 무슨 일이 생겨도 항상 서비스가 가능한 상태, 즉

1. `가용성/내결함성(availability/fault-tolerance)`

의 필요가 절대적이다(가용성을 보장하기 위한 방법은 다음 포스팅에서 알아볼 것이다).

또 무엇이 있을까? 당연할지 모르겠지만, 쾌적한 사용자 경험을 위한 

2. `낮은 지연(low latency)`

일 것이다.

더하여 SNS의 읽기와 쓰기의 비율을 고려했을 때 읽기 요청에 대한 부하가 훨씬 더 클 것이라고 예상할 수 있고, 이런 상황에서 `읽기 요청에 대한 낮은 지연`이 특히 중요할 것이다.


<br>

## 문제 정의

> 타임라인(모아보기) `읽기 초당 450,000 TPS`, 트윗 `쓰기 초당 6,000 TPS`의 대용량 데이터, 대규모 트래픽 상황에서 어떻게 `5초 미만`의 낮은 지연을 항상 보장할 수 있을까?



<br>

# 해결방안1 - Pull vs. Push

가장 먼저, 알고리즘 자체에 집중하기 위해 다음과 같은 전제를 한다.

>
1. 상용 서비스에서 de facto로 여겨지는 disk기반 관계형 DB를 기준으로 한다.
2. 스키마 각 테이블에 대해 적절한 인덱스 설정과 쿼리 최적화가 되어있다고 가정한다.


#### (인덱스에 대하여)
관계형 DB에서의 읽기 성능 개선 방법에는 대표적으로 인덱스 설정이 있다. 인덱스를 설정하면 full-table scan을 방지하고, 인덱스를 설정한 필드에 대해 범위 질의(range scan)을 효율적으로 할 수 있어 읽기 질의 성능을 크게 개선한다. 인덱스 설정시 원본 데이터 중 인덱스가 설정된 필드의 집합을 원본과 별개로 생성하고 정렬하며, 해당 정보를 '페이지'에 기록하고 관리한다. 데이터 삽입시마다 페이지를 갱신하는 연산 비용이 추가되며, 데이터의 규모가 커질 수록 인덱스 페이지 갱신 작업으로 인한 쓰기 성능이 급격히 악화된다는 단점이 있다.

#### (구조적 측면에서의 성능 개선 방법에 대하여)
스케일 업과 스케일 아웃, 데이터 분산처리, 데이터 저장소 선택(디스크 기반 저장소와 인메모리 저장소, SQL과 NoSQL) 등 구조적 성능 개선 방법에 대해선 다음 포스팅에서 다룬다.

#### Pull vs. Push
읽기 성능 보장을 위해 크게 다음의 'pull 방식'과, 'push 방식'의 두 가지 알고리즘을 구상하였다.

## Pull 방식 알고리즘

첫 번째로 떠올린 알고리즘은 가장 전형적인 알고리즘이라고 볼 수 있다.

>
#### 쓰기 시:
1. 트윗을 모든 트윗들이 모여있는 전역 테이블에 바로 쓴다.

#### 읽기 시:
1. 팔로잉하는 유저(이하 팔로잉 유저)를 모두 읽는다.
2. 팔로잉 유저의 트윗을 모두 읽는다. 
3. 시간 순으로 정렬한 트윗을 벌크 로드한다.

흔히 게시판 서비스 페이징을 할 때 이런 방식을 채택한다(데이터를 필요한 당시 DB에서 조회해 '당겨 온다' 하여 'pull 방식'으로 칭하였다).

<br>

## Push 방식 알고리즘

세 번째 생각해낸 알고리즘은 읽기를 요청 시점에 ~~허겁지겁~~ 탐색을 하는 것이 아니라, 필요할 때 즉시 가져올 수 있도록 미리 데이터를 준비 해놓을 수는 없을까?하는 아이디어에서 나왔다.

>
#### 쓰기 시:
1. 각 유저별로 자신만의 타임라인 저장소를 갖고 있다.
2. 트윗을 작성해 쓰기 요청한다.
3. 작성자를 팔로우하는 사람들의 목록을 읽는다.
4. 팔로우 목록의 각 타임라인 저장소에 트윗을 쓴다(fan-out).

#### 읽기 시:
1. 미리 쌓아둔 트윗 목록을 벌크 로드 한다.

이는 쓰기 시점 당시 수신자 각각의 저장소에 데이터를 보내두는 '메일함 서비스'와 비슷하다고 볼 수 있다. 유저의 타임라인(메일함)에 팔로잉 유저들이 쓴 트윗이 이미 정렬 및 저장돼 있고, 필요할 때 트윗 목록을 그대로 가져오기만 하면 된다(데이터를 적소에 미리 '밀어 넣어 둔다'고 하여 'push 방식'이라고 칭하였다).


#### 여담
원본 데이터를 여러 개로 복사해 '환기시키듯 퍼져나가게 한다'는 의미에서, 이런 연산을 흔히 `'fan-out'`이라 칭한다. 한편 데이터 탐색에 드는 연산을 미리 해둔다는 측면에서 `'pre-computing'`, 미리 데이터를 로드해 둔다는 측면에서 `'pre-loading'`, 필요한 데이터를 미리 캐싱해 둔다는 측면에서 `'cache-warming'` 또는 `'look-ahead cache'`라고도 할 수 있다.


## Pull 방식과 Push 방식의 트레이드 오프

Pull 방식과 push 방식 중 하나를 택해야 한다면 다음의 트레이드 오프들을 고려할 수 있다.

### Pull 방식의 트레이드 오프
#### 장점
* 비용대비 moderate한 읽기 성능 제공(가성비)
* 데이터 일관성 확보

#### 단점(비용)
* 읽기 당시에 탐색 비용 지불
* 탐색의 시간 복잡도
* 일정 수준 이상으로 로우 수가 증가할시 인덱스상 쓰기에서 급격한 부하 발생


### Push 방식의 트레이드 오프
#### 장점
* 읽기 속도의 현저한 개선

#### 단점(비용)
* 쓰기 부하 증가
* 중복 데이터로 인한 공간 비용 증가
* 파생 데이터 다수 발생으로 인한 데이터 일관성 보장 난이도 증가
* 애플리케이션 구현 복잡도 증가


<br>

---

## 어떤 쪽을 선택해야할까?

SNS의 읽기 요청 vs. 쓰기 요청 비율상 읽기 성능이 최우선 요구사항임을 고려했을 때, 상술한 추가 비용을 지불하고서라도 push 방식을 도입하는 것이 합리적일 수 있다.

다만 결정에 앞서 추가로 고민해볼 문제가 있다. 트위터의 서비스 특성상 발생하는 문제이다. Pull 방식 선택시 읽는 사람의 팔로잉이 많다면 부하가 과도하게 집중될 수 있다. Push 방식 선택시 쓰는 사람의 팔로워가 많다면 부하가 과도하게 집중될 수 있다.


---
## 문제2 - 트위터 서비스 특성상의 문제: 팔로우

### 팔로잉이 많은 사람의 타임라인 읽기 문제(Pull 방식 선택시 발생)

유저의 팔로잉 분포 범위는 팔로워 분포 범위보다 넓지 않다는 가설은 유효할 것이다. 

예를 들어, 일론 머스크의 팔로워는 1억명이 넘는 반면, 한 유저가 팔로잉하는 수는 대체로 100명에서 많아야 1,000명 수준일 것이다(한 유저가 1억 명의 유저들을 팔로우하진 않는다).

따라서 팔로잉이 많은 사람의 타임라인 읽기는 크게 고려하지 않아도 된다고 치부할 수 있지만, 좀 더 신중히 고려할 이슈가 있다. Pull 방식에서 타임라인 읽기의 시간 복잡도를 생각해보자(user, tweet, follow 테이블의 인덱스가 최적화 되어있다고 가정).

>
O(`f`\*log(`N`) \* log(`N`\*`t`))
* `f`: 한 유저의 팔로잉 수
* `N`: 트위터 전체 유저 수
* `t`: 유저당 평균 트윗 수

`N`과 `t`의 값이 아무리 커진다고 해도 결과는 `로그 증가`인 반면, `f`의 경우 값에 비례해 `선형 증가`한다. 즉, `팔로잉의 수는 타임라인 읽기 연산에 큰 영향을 준다`고 볼 수 있다. 

그렇다면 과연 Pull 방식 채택 경우 모든 타임라인 읽기 요청에 대해 제한시간내 응답을 보장할 수 있을까...?


<br>

### 팔로워가 많은 사람의 트윗 쓰기 문제(Push 방식 선택시 발생)

Push 방식
트위터의 대표적 셀럽 유저 일론 머스크는 1억 명 이상의 팔로워를 보유하고 있다. Push 방식을 구현할 경우 일론머스크가 트윗을 할 때마다 1억 명의 타임라인에 트윗을 1개 씩 총 1억 개를 써야하는 큰 부하가 발생하게 된다.

그렇다면 과연 Push 방식 채택 경우 모든 쓰기 요청에 대해 제한시간내 응답을 보장할 수 있을까...?


<br>

## 추가 해결 방안들

Pull 방식과 push 방식을 혼합하여 문제를 해결하는 방안을 구상하였다.

### 문제 1의 해결:

팔로잉이 많은 사람이 pull 방식의 타임라인 읽기를 할 시 follow 테이블과 tweet 테이블 탐색에서 큰 부하가 발생한다.

따라서 트윗을 작성할시 `쓰는 유저의 팔로워의 팔로잉 수`를 임계치로 삼아, 특정 수 이상인 유저에 대해서만 push 방식의 fan-out을 적용하도록 하여 읽기와 쓰기 부하의 균형을 찾을 수 있다.

이 방식에선 유저가 트윗을 쓸 시 임계치를 넘은 유저(헤비 유저) 목록을 탐색하는 비싼 비용이 발생하는데, 이 때 `헤비 유저 목록`을 캐싱해두면 쓰기시 부하를 추가로 줄일 수 있다.

(팔로우 완전 캐싱)
쓰는 사람 -> 팔로워 중 헤비 유저 목록 캐싱
읽는 라이트 유저 -> 팔로잉 전체 목록 캐싱 -> 읽기시 성능 개선



### 알고리즘3
> #### 쓰기 시
    1. DB에 트윗을 쓴다. 
    2. 팔로워 목록 전체 중 헤비유저 목록을 추려서 불러온다.
    3. 팔로워들 중 헤비유저 목록을 추린다.
    4. 트윗을 헤비유저 목록의 타임라인 저장소에 쓴다.
#### 읽기 시
    1. 본인이 헤비유저에 해당하는지 확인한다.
    2. (본인이 헤비유저일시)파생 저장소에서 모든 트윗을 읽어온다.



<br>

### 문제 2의 해결:

팔로워가 많은 사람이 push 방식의 트윗 쓰기를 할 시 fan-out으로 인해 큰 부하가 발생한다.

따라서 트윗 작성시 `쓰는 유저의 팔로워 수`를 임계치로 삼아, 특정 수 이하인 유저에 대해서만 push 방식의 fan-out을 적용하도록 하여 읽기와 쓰기 부하의 균형을 찾을 수 있다.

이 방식에선 유저가 타임라인을 읽을 시 임계치를 넘은 유저(셀럽) 목록을 탐색하는 비싼 비용이 발생하는데, 이 때 `셀럽 유저 목록`을 캐싱해두면 읽기시 부하를 추가로 줄일 수 있다.

(팔로우 완전 캐싱)
쓰는 사람 -> 팔로워 중 셀럽 유저 목록 캐싱
읽는 사람 -> 팔로잉 중 비셀럽 유저 목록 캐싱


### 알고리즘4
> #### 쓰기 시
    1. DB에 트윗을 쓴다.
    2. 본인이 비셀럽 유저에 해당하는지 확인한다.
    3. (본인이 비셀럽일시)팔로워 목록 전체를 불러온다.
    4. 트윗을 팔로워 목록의 타임라인 저장소에 쓴다.
#### 읽기 시
    1. 미리 저장해둔 비셀럽의 트윗목록을 타임라인 저장소에서 읽는다(유저아이디 하나면 ok).
    2. 셀럽 목록을 파생 저장소에서 읽는다.
    3. 셀럽의 트윗목록을 DB에서 읽는다.
    4. 셀럽, 비셀럽 트윗목록을 병합한다.





## what to choose? 알고리즘 4

알고리즘 3과 4는 기본적으로 비슷한 읽기와 쓰기 부하를 주도록 설계할 수 있다.

단지 알고리즘 3의 경우 팔로잉의 임계치를, 알고리즘 4의 경우 팔로워의 임계치를 어떻게 설정하는지에 따라 읽기와 쓰기의 부하 분산이 가능해진다.

나는 4의 알고리즘을 선택하였는데, 이유는 
* 임계치로 설정할 값(팔로워 수)의 표본의 분포가 더 다양하기 때문에 임계치 설정에 있어 보다 세밀한 설정이 가능할 것이며, 
* 셀럽인지 비셀럽인지에 따라 fan-out 쓰기 여부를 결정하는 것이 시스템 구현에 있어 보다 직관적이라고 판단하였다.






## 구현 코드 및 시퀀스 다이어그램 삽입





그 외 구현에 있어 세부사항으로 읽기 요청 당시 셀럽의 트윗의 경우 셀럽 유저목록을 불러와야 하는데(위에서 서술했듯 팔로잉의 수가 늘어나는 것은 타임라인 읽기 연산에 상대적으로 큰 영향을 준다), 그를 위해서 읽기 요청 당시 DB에서 셀럽 팔로잉 목록을 조회하는 연산을 줄이기 위해 팔로우중 셀럽인 유저목록도 타임라인과 같이 따로 저장해둘 수 있도록 구현하였다.

위 구현은 읽기 성능을 향상시키는데, 추가로 전체 팔로워 목록을 캐싱하면 쓰기시의 성능 또한 크게 개션할 수 있을 것이다.




## reference

1.https://www.statista.com/statistics/970920/monetizable-daily-active-twitter-users-worldwide/
2. 데이터 중심 애플리케이션 설계
3. https://www.dsayce.com/social-media/tweets-day/










——
링크드인 프로필 읽기 요청과 다르게 트위터의 타임라인은 실시간으로 변하기에 훨씬 더 큰 복잡성을 갖는다.


——
팔로잉의 분포 범위가 넓지 않은 상황에서 팔로잉이 많은 사람을 특정 기준으로 나누고 해당 유저에 대해 별도의 팔로잉 목록을 미리 저장해 두는 것은 오히려 불필요하게 애플리케이션 복잡도를 증가시킨다고 판단했다.

예를 들어 push 방식으로 미리 타임라인을 저장해 놓으면, 읽기 요청이 들어왔을 때 비셀럽 유저의 목록을 불러올 일이 적다.

반대로 셀럽 유저에 대해선 읽기시 셀럽 유저 목록이 필요하게 되는데, 이 목록은 미리 캐싱을 해두는 방식으로 읽기시 부하를 최소화 시킬 수 있는 것이다.

(쓰기시 비셀럽 유저의 팔로잉 목록을 미리 저장해두면 쓰기시 부하를 줄일 수 있겠지만, 우선 읽기 요청 성능에 집중하기 위해 해당 부분을 구현하진 않았다)



——
전자의 범위가 한 유저가 1억 명의 유저를 팔로우하진 않는다. 대부분의 경우, 많이 팔로우를 해봤자 수 십, 수 백 정도에 그칠 것이다. 



——
타임라인의 시간복잡도 분석

알고리즘1
팔로우를 찾는다(logF) + 트윗들을 찾는다(logT)

알고리즘2

그보단 알고리즘 2의 경우 팔로워의 분포와 더 큰 관련이 있다.



———
추론

변수가 있다.

팔로우의 분포이다.

문제: 팔로우 수의 분포는 1억 명까지 분포가 매우 다양하다. (일론머스크의 팔로워는 1억5천만 명이 넘는다)

1억 6천만 명 팔로워 보유 셀럽의 트윗 읽기 요청 집중을 어떻게 감당할까?



———
트위터는 초당 6천 쓰기를 하는 대용량 데이터가 발생하며, 이런 대용량 데이터의 홍수 속에서 읽기 성능을 보장하는 데에 전통적인 접근법으로는 한계가 있을 것이라고 판단했다.

또한 극한의 대용량 데이터 속에서 쓰기가 너무 많은 부하를 주게된다.

다만 위에 기술한대로 SNS의 통상적인 요구사항상 쓰기보다 읽기 성능에 대한 요구가 더 크기에 알고리즘 2를 우선해 적용해야 겠다고 생각했다.

극한의 대용량 데이터 처리상 RDB 성능에 한계가 있다...

RDB는 확장에 좀 더 취약하다. 확장성보다는 일관성을 최우선으로 하여 설계되었다. 네트워크가 유실되었을 때 C와 A를 우선해 보장하고, 확장성은 포기한다. (근데 확장성 없이 A내결함성이 가능이나 한가..?)

알고리즘 2을 선택했다면 추가로 고민해 봐야하는 변수가 있다. 바로 팔로우 기능이다.


———
문제2의 해결은 알고리즘1일 때 크게 유효하다. 
문제1의 해결과 문제2의 해결은 양립하기 어렵다.

예를 들어 알고리즘2의 방식 도입시엔 쓰기 시점에 팔로잉 목록을 불러와야 하는데, 이 때 이미 작성자의 팔로워 수를 기준으로 독자들의 타임라인을 채워놓는다.

그렇다면 독자들이 읽기 요청시에 알아야할 것은 셀럽에 해당하는 유저의 목록들만 DB에 조회하면 된다.

독자의 읽기 시점에 미리 저장된 팔로잉 목록을 조회하는 것이 아니란 것이다.



———
다만 push 방식으로 결정한다고 하더라도, 트위터의 현행 통계에서 오는 문제들이 남아있다. 바로 '팔로우' 기능과 관련된 문제이다. 일론머스크는 1억 명 이상의 팔로워를 보유하고 있는데 이런 셀럽들이 트윗을 쓸 때마다 1억 개의 중복데이터 쓰기 부하를 감당할 수 있을까?




———
방법2

팔로잉 수가 특정 임계치를 넘은 유저를 대상으로 팔로잉 목록을 미리 저장해둔다.

쓰는 사람의 팔로워 기준
읽는 사람의 팔로잉 기준


———
팔로잉 목록이 특정 임계치를 넘은 사람의 트윗은 미리 저장해둔다?
읽기 요청시 미리 저장해둔 팔로잉 목록을 불러와 RDB에 읽기 요청을 한다.

팔로잉 쓰기시
임계치를 확인한다
임계치를 넘으면 팔로잉 많은 사람으로 따로 분류하여 팔로잉 목록을 미리 저장해둔다.


———

그 외 구현에 있어 세부사항으로 읽기 요청 당시 셀럽의 트윗의 경우 셀럽 유저목록을 불러와야 하는데(위에서 서술했듯 팔로잉의 수가 늘어나는 것은 타임라인 읽기 연산에 상대적으로 큰 영향을 준다), 그를 위해서 읽기 요청 당시 DB에서 셀럽 팔로잉 목록을 조회하는 연산을 줄이기 위해 팔로우중 셀럽인 유저목록도 타임라인과 같이 따로 저장해둘 수 있도록 구현하였다.